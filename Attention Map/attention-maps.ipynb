{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob \nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom skimage.util import montage as montage2d\nfrom skimage.io import imread\nbase_dir = os.path.join('..', 'input', 'pulmonary-chest-xray-abnormalities')\nall_xray_df = pd.read_csv('../input/data/Data_Entry_2017.csv')\nall_xray_df.sample(5)","metadata":{"_cell_guid":"c3cc4285-bfa4-4612-ac5f-13d10678c09a","_uuid":"725d378daf5f836d4885d67240fc7955f113309d","execution":{"iopub.status.busy":"2022-04-02T10:50:17.400615Z","iopub.execute_input":"2022-04-02T10:50:17.401008Z","iopub.status.idle":"2022-04-02T10:50:17.620538Z","shell.execute_reply.started":"2022-04-02T10:50:17.400949Z","shell.execute_reply":"2022-04-02T10:50:17.618559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_xray_df = pd.read_csv('../input/data/Data_Entry_2017.csv')\nall_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..', 'input', 'data',  'images*', '*', '*.png'))}\nprint('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\nall_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\nall_xray_df['Cardiomegaly'] = all_xray_df['Finding Labels'].map(lambda x: 'Cardiomegaly' in x)\nall_xray_df['Patient Age'] = np.clip(all_xray_df['Patient Age'], 5, 100)\nall_xray_df['Patient Male'] = all_xray_df['Patient Gender'].map(lambda x: x.upper()=='M').astype('float32')\nall_xray_df.sample(3)","metadata":{"_cell_guid":"1d79959c-4921-48f9-a660-f1d550cf3b1d","_uuid":"2a7d23fce33af0de54202048f00fcd23e9d876c9","execution":{"iopub.status.busy":"2022-04-02T10:50:17.755339Z","iopub.execute_input":"2022-04-02T10:50:17.755983Z","iopub.status.idle":"2022-04-02T10:50:18.729991Z","shell.execute_reply.started":"2022-04-02T10:50:17.755916Z","shell.execute_reply":"2022-04-02T10:50:18.728718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(all_xray_df[['Patient Age', 'Patient Male', 'Cardiomegaly']], hue='Cardiomegaly')","metadata":{"_cell_guid":"5c8bd288-8261-4cbe-a954-e62ac795cc3e","_uuid":"60a8111c4093ca6f69d27a4499442ba7dd750839","execution":{"iopub.status.busy":"2022-04-02T10:50:18.733183Z","iopub.execute_input":"2022-04-02T10:50:18.733777Z","iopub.status.idle":"2022-04-02T10:50:36.373538Z","shell.execute_reply.started":"2022-04-02T10:50:18.733545Z","shell.execute_reply":"2022-04-02T10:50:36.372392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data into Training and Validation","metadata":{"_cell_guid":"0ba697ed-85bb-4e9a-9765-4c367db078d1","_uuid":"4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d"}},{"cell_type":"code","source":"positive_cases = np.sum(all_xray_df['Cardiomegaly']==True)//2\noversample_factor = 4 # maximum number of cases in negative group so it isn't super rare\nmore_balanced_df = all_xray_df.groupby(['Patient Gender', 'Cardiomegaly']).apply(lambda x: x.sample(min(oversample_factor*positive_cases, x.shape[0]), \n                                                                                   replace = False)\n                                                      ).reset_index(drop = True)\n\nprint(more_balanced_df['Cardiomegaly'].value_counts())\nsns.pairplot(more_balanced_df[['Patient Age', 'Cardiomegaly']], hue='Cardiomegaly')","metadata":{"_uuid":"5ee6e4a8277bf42ffac3a5af2892664441879d6b","execution":{"iopub.status.busy":"2022-04-02T10:50:36.375248Z","iopub.execute_input":"2022-04-02T10:50:36.375938Z","iopub.status.idle":"2022-04-02T10:50:39.107032Z","shell.execute_reply.started":"2022-04-02T10:50:36.375862Z","shell.execute_reply":"2022-04-02T10:50:39.106029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nraw_train_df, test_valid_df = train_test_split(more_balanced_df, \n                                   test_size = 0.30, \n                                   random_state = 2018,\n                                   stratify = more_balanced_df[['Cardiomegaly', 'Patient Gender']])\nvalid_df, test_df = train_test_split(test_valid_df, \n                                   test_size = 0.40, \n                                   random_state = 2018,\n                                   stratify = test_valid_df[['Cardiomegaly', 'Patient Gender']])\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0], 'test', test_df.shape[0])\nprint('train', raw_train_df['Cardiomegaly'].value_counts())\nprint('test', test_df['Cardiomegaly'].value_counts())\nraw_train_df.sample(1)","metadata":{"_cell_guid":"1192c6b3-a940-4fa0-a498-d7e0d400a796","_uuid":"a48b300ca4d37a6e8b39f82e3c172739635e4baa","execution":{"iopub.status.busy":"2022-04-02T10:50:39.108801Z","iopub.execute_input":"2022-04-02T10:50:39.109462Z","iopub.status.idle":"2022-04-02T10:50:39.329161Z","shell.execute_reply.started":"2022-04-02T10:50:39.109384Z","shell.execute_reply":"2022-04-02T10:50:39.328283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Balancing distribution","metadata":{"_cell_guid":"f8060459-da1e-4293-8f61-c7f99de1de9f","_uuid":"26e566d6cec5bd41f9afe392f456ddf7ceb306ea"}},{"cell_type":"code","source":"train_df = raw_train_df.groupby(['Cardiomegaly']).apply(lambda x: x.sample(2000, replace = True)\n                                                      ).reset_index(drop = True)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])","metadata":{"_cell_guid":"21b5d30f-c645-41ad-85bc-4b51d237c950","_uuid":"dba4c54209e5ade39d771a7f5d9d995321f0a367","execution":{"iopub.status.busy":"2022-04-02T10:50:39.334807Z","iopub.execute_input":"2022-04-02T10:50:39.335092Z","iopub.status.idle":"2022-04-02T10:50:39.362083Z","shell.execute_reply.started":"2022-04-02T10:50:39.335036Z","shell.execute_reply":"2022-04-02T10:50:39.361225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\nfrom PIL import Image\nIMG_SIZE = (512, 512) # slightly smaller than vgg16 normally expects\ncore_idg = ImageDataGenerator(samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip=False, \n                              vertical_flip=False, \n                              height_shift_range=0.1, \n                              width_shift_range=0.1, \n                              brightness_range=[0.7, 1.5],\n                              rotation_range=3, \n                              shear_range=0.01,\n                              fill_mode='nearest',\n                              zoom_range=0.125,\n                             preprocessing_function=preprocess_input)","metadata":{"_cell_guid":"9954bfda-29bd-4c4d-b526-0a972b3e43e2","_uuid":"9529ab766763a9f122786464c24ab1ebe22c6006","execution":{"iopub.status.busy":"2022-04-02T10:50:39.365863Z","iopub.execute_input":"2022-04-02T10:50:39.366477Z","iopub.status.idle":"2022-04-02T10:50:39.375263Z","shell.execute_reply.started":"2022-04-02T10:50:39.366176Z","shell.execute_reply":"2022-04-02T10:50:39.373872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n    df_gen = img_data_gen.flow_from_dataframe(in_df,\n                                              x_col=path_col,\n                                              y_col=y_col,\n                                     class_mode = 'raw',\n                                    **dflow_args)\n    return df_gen","metadata":{"_cell_guid":"b5767f42-da63-4737-8f50-749c1a25aa84","_uuid":"07851e798db3d89ba13db7d4b56ab2b759221464","execution":{"iopub.status.busy":"2022-04-02T10:50:39.377095Z","iopub.execute_input":"2022-04-02T10:50:39.377655Z","iopub.status.idle":"2022-04-02T10:50:39.385805Z","shell.execute_reply.started":"2022-04-02T10:50:39.377596Z","shell.execute_reply":"2022-04-02T10:50:39.384545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'path',\n                            y_col = 'Cardiomegaly', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 8)\n\nvalid_gen = flow_from_dataframe(core_idg, valid_df, \n                             path_col = 'path',\n                            y_col = 'Cardiomegaly', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 256) # we can use much larger batches for evaluation\n# used a fixed dataset for evaluating the algorithm\ntest_X, test_Y = next(flow_from_dataframe(core_idg, \n                               valid_df, \n                             path_col = 'path',\n                            y_col = 'Cardiomegaly', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 400)) # one big batch\n# used a fixed dataset for final evaluation\nfinal_test_X, final_test_Y = next(flow_from_dataframe(core_idg, \n                               test_df, \n                             path_col = 'path',\n                            y_col = 'Cardiomegaly', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 400)) # one big batch","metadata":{"_cell_guid":"810bd229-fec9-43c4-b3bd-afd62e3e9552","_uuid":"1848f5048a9e00668c3778a85deea97f980e4f1c","execution":{"iopub.status.busy":"2022-04-02T10:50:39.387733Z","iopub.execute_input":"2022-04-02T10:50:39.388921Z","iopub.status.idle":"2022-04-02T10:52:10.173240Z","shell.execute_reply.started":"2022-04-02T10:50:39.388334Z","shell.execute_reply":"2022-04-02T10:52:10.172315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -127, vmax = 127)\n    c_ax.set_title('%s' % ('Cardiomegaly' if c_y>0.5 else 'Healthy'))\n    c_ax.axis('off')","metadata":{"_cell_guid":"2d62234f-aeb0-4eba-8a38-d713d819abf6","_uuid":"8190b4ad60d49fa65af074dd138a19cb8787e983","execution":{"iopub.status.busy":"2022-04-02T10:52:10.175033Z","iopub.execute_input":"2022-04-02T10:52:10.175366Z","iopub.status.idle":"2022-04-02T10:52:11.794717Z","shell.execute_reply.started":"2022-04-02T10:52:10.175308Z","shell.execute_reply":"2022-04-02T10:52:11.793864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrained Features\n","metadata":{"_cell_guid":"da22790a-672c-474e-b118-9eef15b53160","_uuid":"55d665e1e8a8d83b9db005a66a965f8a90c62da1"}},{"cell_type":"code","source":"base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \n                              include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False","metadata":{"_cell_guid":"fd7b325e-a5cf-4305-aa0a-ac5d1a8cb4dc","_uuid":"2d0b66df0294c563bc7d6d8cdd09b936a904981b","execution":{"iopub.status.busy":"2022-04-02T10:52:11.796041Z","iopub.execute_input":"2022-04-02T10:52:11.796511Z","iopub.status.idle":"2022-04-02T10:52:12.919652Z","shell.execute_reply.started":"2022-04-02T10:52:11.796425Z","shell.execute_reply":"2022-04-02T10:52:12.918628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Model\n","metadata":{"_cell_guid":"731c14eb-5db3-4f89-b433-d372cc95ec36","_uuid":"2aef067b87e53e1aa9490d86cdf10bcbc2398303"}},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, AvgPool2D\nfrom keras.models import Model\npt_features = Input(base_pretrained_model.get_output_shape_at(0)[1:], name = 'feature_input')\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\nfrom keras.layers import BatchNormalization\nbn_features = BatchNormalization(name='Features_BN')(pt_features)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T10:52:12.925318Z","iopub.execute_input":"2022-04-02T10:52:12.927982Z","iopub.status.idle":"2022-04-02T10:52:13.071156Z","shell.execute_reply.started":"2022-04-02T10:52:12.927917Z","shell.execute_reply":"2022-04-02T10:52:13.070125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention\n","metadata":{}},{"cell_type":"code","source":"attn_layer = Conv2D(128, kernel_size = (1,1), padding = 'same', activation = 'elu')(bn_features)\nattn_layer = Conv2D(32, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\nattn_layer = AvgPool2D((2,2), strides = (1,1), padding = 'same')(attn_layer) # smooth results\nattn_layer = Conv2D(1, \n                    kernel_size = (1,1), \n                    padding = 'valid', \n                    activation = 'sigmoid',\n                   name='AttentionMap2D')(attn_layer)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T10:52:13.076751Z","iopub.execute_input":"2022-04-02T10:52:13.079442Z","iopub.status.idle":"2022-04-02T10:52:13.177161Z","shell.execute_reply.started":"2022-04-02T10:52:13.079382Z","shell.execute_reply":"2022-04-02T10:52:13.176236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rescale Attention\n","metadata":{}},{"cell_type":"code","source":"# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', name='UpscaleAttention',\n               activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T10:52:13.182052Z","iopub.execute_input":"2022-04-02T10:52:13.184627Z","iopub.status.idle":"2022-04-02T10:52:13.600166Z","shell.execute_reply.started":"2022-04-02T10:52:13.184547Z","shell.execute_reply":"2022-04-02T10:52:13.599271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global Weighted Average Pooling\nWe now want to use the attention layer to weight the regions we want during the average pooling. A standard average pooling layer is poorly suited to this task since many of the values (presumably) will be zero and they will be counted. So we hand-rig a 'weighted average pooling' where we multiply the attention by the features and then divide by the sum of the attention\nThe formula for weighted average from [Wikipedia](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean)\n$$ \\bar{x} = \\frac{ \\sum\\limits_{i=1}^n w_i x_i}{\\sum\\limits_{i=1}^n w_i} $$\n\n$$ \\text{GWAP}(x, y, d) = \\frac{ \\sum\\limits_{x}\\sum\\limits_{y} \\text{Attention}(x,y,d) \\text{Feature}(x,y,d)} {\\sum\\limits_{x}\\sum\\limits_{y} \\text{Attention}(x,y,d)} $$","metadata":{}},{"cell_type":"code","source":"mask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T10:52:13.603135Z","iopub.execute_input":"2022-04-02T10:52:13.603498Z","iopub.status.idle":"2022-04-02T10:52:13.619278Z","shell.execute_reply.started":"2022-04-02T10:52:13.603425Z","shell.execute_reply":"2022-04-02T10:52:13.618272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropout and Classification\nWe know take the output of this global weighted-average pooling and go to a classification with dropout and two fully connected layers","metadata":{}},{"cell_type":"code","source":"gap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.5)(Dense(128, activation = 'elu')(gap_dr))\nout_layer = Dense(1, activation = 'sigmoid')(dr_steps)\n\nattn_model = Model(inputs = [pt_features], outputs = [out_layer], name = 'attention_model')\n\nattn_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n                           metrics = ['binary_accuracy'])\n\nattn_model.summary()","metadata":{"_cell_guid":"eeb36110-0cde-4450-a43c-b8f707adb235","_uuid":"1f0dfaccda346d7bc4758e7329d61028d254a8d6","execution":{"iopub.status.busy":"2022-04-02T10:52:13.621326Z","iopub.execute_input":"2022-04-02T10:52:13.622271Z","iopub.status.idle":"2022-04-02T10:52:13.760395Z","shell.execute_reply.started":"2022-04-02T10:52:13.621677Z","shell.execute_reply":"2022-04-02T10:52:13.759533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture\nWe can show the whole model below","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import Image\nImage(model_to_dot(attn_model, show_shapes=True).create_png())","metadata":{"execution":{"iopub.status.busy":"2022-04-02T10:52:13.765817Z","iopub.execute_input":"2022-04-02T10:52:13.766304Z","iopub.status.idle":"2022-04-02T10:52:14.213496Z","shell.execute_reply.started":"2022-04-02T10:52:13.766245Z","shell.execute_reply":"2022-04-02T10:52:14.212560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('cardio_attn')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=10) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","metadata":{"_cell_guid":"17803ae1-bed8-41a4-9a2c-e66287a24830","_uuid":"48b9764e16fb5af52aed35c82bae6299e67d5bc7","execution":{"iopub.status.busy":"2022-04-02T10:52:14.215586Z","iopub.execute_input":"2022-04-02T10:52:14.216317Z","iopub.status.idle":"2022-04-02T10:52:14.228337Z","shell.execute_reply.started":"2022-04-02T10:52:14.216238Z","shell.execute_reply":"2022-04-02T10:52:14.227289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# the model\nWe build the whole model and fine-tune the results (much lower learning rate)","metadata":{"_cell_guid":"c3ca0202-2e71-4697-8317-8c199ec6fad9","_uuid":"b9d61cdbca47129efb888ef68b75659d696b8760"}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.optimizers import Adam\ntb_model = Sequential(name = 'combined_model')\nbase_pretrained_model.trainable = False\ntb_model.add(base_pretrained_model)\ntb_model.add(attn_model)\ntb_model.compile(optimizer = Adam(lr = 1e-3), loss = 'binary_crossentropy',\n                           metrics = ['binary_accuracy'])\ntb_model.summary()","metadata":{"_cell_guid":"a5669eb8-704a-4f13-8520-42fd6d458d68","_uuid":"4ae9b74e038a84a98064143c18ee2273fec62a23","execution":{"iopub.status.busy":"2022-04-02T10:52:14.230037Z","iopub.execute_input":"2022-04-02T10:52:14.230785Z","iopub.status.idle":"2022-04-02T10:52:14.696660Z","shell.execute_reply.started":"2022-04-02T10:52:14.230649Z","shell.execute_reply":"2022-04-02T10:52:14.694200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen.batch_size = 24\ntb_model.fit_generator(train_gen, \n                      validation_data = (test_X, test_Y), \n                        steps_per_epoch=train_gen.n//train_gen.batch_size,\n                      epochs = 30, \n                      callbacks = callbacks_list,\n                      workers = 3)","metadata":{"_cell_guid":"2b7d13e0-0517-46ea-9227-fce26720d981","_uuid":"f7444fd153779e3bd8fbc7cdb4ffa4b07f87de05","execution":{"iopub.status.busy":"2022-04-02T10:52:14.701142Z","iopub.execute_input":"2022-04-02T10:52:14.701428Z","iopub.status.idle":"2022-04-02T12:10:44.874488Z","shell.execute_reply.started":"2022-04-02T10:52:14.701371Z","shell.execute_reply":"2022-04-02T12:10:44.873568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tb_model.load_weights(weight_path)","metadata":{"_uuid":"683a5a02c86339d98d19db6235ae43ef376cda3e","execution":{"iopub.status.busy":"2022-04-02T12:10:44.879311Z","iopub.execute_input":"2022-04-02T12:10:44.882627Z","iopub.status.idle":"2022-04-02T12:10:45.732820Z","shell.execute_reply.started":"2022-04-02T12:10:44.882559Z","shell.execute_reply":"2022-04-02T12:10:45.731109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Attention\n","metadata":{"_cell_guid":"11f33f0a-61eb-488a-b7ea-4bc9d15ba8f9","_uuid":"cca170eb40bc591f89748ede8aa35de4308faaaf"}},{"cell_type":"code","source":"# get the attention layer since it is the only one with a single output dim\nfor attn_layer in attn_model.layers:\n    c_shape = attn_layer.get_output_shape_at(0)\n    if len(c_shape)==4:\n        if c_shape[-1]==1:\n            print(attn_layer)\n            break","metadata":{"_cell_guid":"e41a063f-35c9-410f-be63-f66b63ff9683","_uuid":"ad5b085d351e79b950bf0c2ddc476799d5b0692f","execution":{"iopub.status.busy":"2022-04-02T12:10:45.734817Z","iopub.execute_input":"2022-04-02T12:10:45.735195Z","iopub.status.idle":"2022-04-02T12:10:45.751298Z","shell.execute_reply.started":"2022-04-02T12:10:45.735127Z","shell.execute_reply":"2022-04-02T12:10:45.749883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\nrand_idx = np.random.choice(range(len(test_X)), size = 3)\nattn_func = K.function(inputs = [attn_model.get_input_at(0), K.learning_phase()],\n           outputs = [attn_layer.get_output_at(0)]\n          )\nfig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n    cur_img = test_X[c_idx:(c_idx+1)]\n    cur_features = base_pretrained_model.predict(cur_img)\n    attn_img = attn_func([cur_features, 0])[0]\n    img_ax.imshow(cur_img[0,:,:,0], cmap = 'bone')\n    attn_ax.imshow(attn_img[0, :, :, 0], cmap = 'viridis', \n                   vmin = 0, vmax = 1, \n                   interpolation = 'lanczos')\n    real_label = test_Y[c_idx]\n    img_ax.set_title('Cardio\\nClass:%s' % (real_label))\n    pred_confidence = tb_model.predict(cur_img)[0]\n    attn_ax.set_title('Attention Map\\nPred:%2.1f%%' % (100*pred_confidence[0]))\nfig.savefig('attention_map.png', dpi = 300)","metadata":{"_cell_guid":"340eef36-f5b2-4b15-a59f-440061a427eb","_uuid":"00850972ae4298f49ed1838b3fc49c2d8fb07547","execution":{"iopub.status.busy":"2022-04-02T12:10:45.753416Z","iopub.execute_input":"2022-04-02T12:10:45.754069Z","iopub.status.idle":"2022-04-02T12:10:51.662655Z","shell.execute_reply.started":"2022-04-02T12:10:45.753995Z","shell.execute_reply":"2022-04-02T12:10:51.661753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_Y = tb_model.predict(test_X, \n                          batch_size = 32, \n                          verbose = True)","metadata":{"_cell_guid":"d0edaf00-4b7c-4f65-af0b-e5a03b9b8428","_uuid":"b421b6183b1919a7414482f0b1ac611079e45174","execution":{"iopub.status.busy":"2022-04-02T12:10:51.664115Z","iopub.execute_input":"2022-04-02T12:10:51.664654Z","iopub.status.idle":"2022-04-02T12:10:58.199596Z","shell.execute_reply.started":"2022-04-02T12:10:51.664603Z","shell.execute_reply":"2022-04-02T12:10:58.198615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nplt.matshow(confusion_matrix(test_Y, pred_Y>0.5))\nprint(classification_report(test_Y, pred_Y>0.5, target_names = ['Healthy', 'Cardiomegaly']))","metadata":{"_cell_guid":"15189df2-3fed-495e-9661-97bb2b712dfd","_uuid":"10162e055ca7cd52878a289bab377231787ab732","execution":{"iopub.status.busy":"2022-04-02T12:10:58.203245Z","iopub.execute_input":"2022-04-02T12:10:58.203782Z","iopub.status.idle":"2022-04-02T12:10:58.383508Z","shell.execute_reply.started":"2022-04-02T12:10:58.203725Z","shell.execute_reply":"2022-04-02T12:10:58.382473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, _ = roc_curve(test_Y, pred_Y)\nfig, ax1 = plt.subplots(1,1, figsize = (5, 5), dpi = 250)\nax1.plot(fpr, tpr, 'b.-', label = 'VGG-Model (AUC:%2.2f)' % roc_auc_score(test_Y, pred_Y))\nax1.plot(fpr, fpr, 'k-', label = 'Random Guessing')\nax1.legend(loc = 4)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate');\nfig.savefig('roc.pdf')","metadata":{"_cell_guid":"c0a16fc6-5625-49c2-8070-a724e2ee0708","_uuid":"8cd2cbe199c74a14f9ca736ac8e0b5d12353a51c","execution":{"iopub.status.busy":"2022-04-02T12:10:58.388978Z","iopub.execute_input":"2022-04-02T12:10:58.392012Z","iopub.status.idle":"2022-04-02T12:10:59.060451Z","shell.execute_reply.started":"2022-04-02T12:10:58.389295Z","shell.execute_reply":"2022-04-02T12:10:59.059499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Results\n","metadata":{"_uuid":"bb745208886d1e143587056ed2b2fdcc3ef155f8"}},{"cell_type":"code","source":"final_pred_Y = tb_model.predict(final_test_X, \n                                verbose = True, \n                                batch_size = 4)","metadata":{"_uuid":"8dd44836d313d6c9034a95084713e343d991c207","execution":{"iopub.status.busy":"2022-04-02T12:10:59.062206Z","iopub.execute_input":"2022-04-02T12:10:59.062858Z","iopub.status.idle":"2022-04-02T12:11:05.745895Z","shell.execute_reply.started":"2022-04-02T12:10:59.062796Z","shell.execute_reply":"2022-04-02T12:11:05.744865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.matshow(confusion_matrix(final_test_Y, final_pred_Y>0.5))\nprint(classification_report(final_test_Y, final_pred_Y>0.5, target_names = ['Healthy', 'Cardiomegaly']))","metadata":{"_uuid":"ad51a93896e32d2fed8b3f05c97514a58dc4ce7c","execution":{"iopub.status.busy":"2022-04-02T12:11:05.751100Z","iopub.execute_input":"2022-04-02T12:11:05.752497Z","iopub.status.idle":"2022-04-02T12:11:06.068716Z","shell.execute_reply.started":"2022-04-02T12:11:05.752127Z","shell.execute_reply":"2022-04-02T12:11:06.067467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, _ = roc_curve(final_test_Y, final_pred_Y)\nfig, ax1 = plt.subplots(1,1, figsize = (5, 5), dpi = 250)\nax1.plot(fpr, tpr, 'b.-', label = 'VGG-Model (AUC:%2.2f)' % roc_auc_score(test_Y, pred_Y))\nax1.plot(fpr, fpr, 'k-', label = 'Random Guessing')\nax1.legend(loc = 4)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate');\nfig.savefig('roc.pdf')","metadata":{"_uuid":"cad0d9ab02b2073eabc8a596c8b91e17cfe38bde","execution":{"iopub.status.busy":"2022-04-02T12:11:06.074516Z","iopub.execute_input":"2022-04-02T12:11:06.077732Z","iopub.status.idle":"2022-04-02T12:11:06.899416Z","shell.execute_reply.started":"2022-04-02T12:11:06.074876Z","shell.execute_reply":"2022-04-02T12:11:06.898587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tb_model.save('full_pred_model.h5')","metadata":{"_cell_guid":"91a6cac6-ced4-4f1d-a52b-2ea998c86eec","_uuid":"583fc085ff387b4227c3a45208dfa0a019521f3f","execution":{"iopub.status.busy":"2022-04-02T12:11:06.901302Z","iopub.execute_input":"2022-04-02T12:11:06.901702Z","iopub.status.idle":"2022-04-02T12:11:07.252771Z","shell.execute_reply.started":"2022-04-02T12:11:06.901644Z","shell.execute_reply":"2022-04-02T12:11:07.251539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exporter","metadata":{"_cell_guid":"6b0339ad-29d7-4570-aadc-7b7cec86fc94","_uuid":"b57642e878974a134e1e84a28b783d526179a650"}},{"cell_type":"code","source":"img_in = Input(t_x.shape[1:])\nfeat_lay = base_pretrained_model(img_in)\njust_attn = Model(inputs = attn_model.get_input_at(0), \n      outputs = [attn_layer.get_output_at(0)], name = 'pure_attention')\nattn_img = just_attn(feat_lay)\npure_attn_model = Model(inputs = [img_in], outputs = [attn_img], name = 'just_attention_model')\npure_attn_model.save('pure_attn_model.h5')\npure_attn_model.summary()","metadata":{"_cell_guid":"cf435072-260f-4e7e-bef8-69fcd8b2a430","_uuid":"52a41375509688c835bb0040f9631f56ecad360d","execution":{"iopub.status.busy":"2022-04-02T12:11:07.255304Z","iopub.execute_input":"2022-04-02T12:11:07.256116Z","iopub.status.idle":"2022-04-02T12:11:07.903685Z","shell.execute_reply.started":"2022-04-02T12:11:07.255687Z","shell.execute_reply":"2022-04-02T12:11:07.902620Z"},"trusted":true},"execution_count":null,"outputs":[]}]}